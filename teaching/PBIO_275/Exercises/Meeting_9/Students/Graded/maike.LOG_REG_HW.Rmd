---
title: "Bayesian Logistic Regression"
author: "Maike Holthuijzen"
date: "November 30, 2017"
output: pdf_document
---

First, I used the \texttt{optim} function in R to estimate parameters for the logistic regression model. The function \texttt{mle.logreg} below estimates parameters using maximum likelihood.
```{r}
#simulate data
set.seed(57)
beta0<- -1.0; beta1<-3.0
light<-runif(1000)
pVect<-exp(beta0+beta1*light)/(1+exp(beta0+beta1*light))
range(pVect)
seedlingSurv<-rbinom(n=length(light),size=1,prob=pVect)

seedlingSurvData<-data.frame(light,seedlingSurv) # if you want a data frame
plot(light,seedlingSurv)


head(seedlingSurvData)

#solve using maximum liklihood
mle.logreg = function(fmla, data)
{
  # negative log likelihood function
  logl <- function(theta,x,y){
    y <- y
    x <- as.matrix(x)
    beta <- theta[1:ncol(x)]
    
    # Use the log-likelihood of the Bernoulli distribution, where p is
    # defined as the logistic transformation of a linear combination
    loglik <- sum(-y*log(1 + exp(-(x%*%beta))) - (1-y)*log(1 + exp(x%*%beta)))
    return(-loglik)
  }
  
  # Prepare the data
  outcome = rownames(attr(terms(fmla),"factors"))[1]
  dfrTmp = model.frame(data)
  x = as.matrix(model.matrix(fmla, data=dfrTmp))
  y = as.numeric(as.matrix(data[,match(outcome,colnames(data))]))
  
  # Define initial values for the parameters
  theta.start = rep(0,(dim(x)[2]))
  names(theta.start) = colnames(x)
  
  # Calculate the maximum likelihood
  mle = optim(theta.start,logl,x=x,y=y,hessian=T)
  out = list(beta=mle$par,vcov=solve(mle$hessian),ll=2*mle$value)
}

fmla = as.formula("seedlingSurv~light")
mylogit = mle.logreg(fmla, seedlingSurvData) #Estimate coefficients

```
The parameter estimates for the intercept (\texttt{beta0}) and coefficient for $\beta_{1}$ are
```{r}
mylogit
```

The estimates are fairly close to the actual values (-1 and 3 for the slope and intercept, respectively.). The plot below shows a plot of the fitted data.
```{r}
lightPreds<-light[order(light)]
plot(light,seedlingSurv)
outresult<-exp(mylogit[[1]][1]+mylogit[[1]][2]*light)/(1+exp(mylogit[[1]][1]+mylogit[[1]][2]*light))

seedlingSurvresult<-rbinom(n=length(light),size=1,prob=outresult)

plot(light,seedlingSurvresult, pch = 19, col = "blue", main = "Predicted values for logistic regression model")
f = function (x) 1 / -(1 + exp(mylogit[[1]][1] + mylogit[[1]][2] * x))
f2 = function (x) exp(mylogit[[1]][1]+mylogit[[1]][2]*x)/(1+exp(mylogit[[1]][1]+mylogit[[1]][2]*x))

lines(lightPreds, f2(lightPreds), col = "red", lwd= 2)
```

We can also compare these results to the output from R's \texttt{glm} function.
```{r}
#compare to glm output
outGlmLogReg = glm(seedlingSurv~light, family=binomial,data=seedlingSurvData)
summary(outGlmLogReg)
```


Next, I used the \texttt{brms} library for the logistic regression model. Again, the results are about the same as those obtained from the maximum likelihood estimates.
```{r , warning=FALSE, echo=FALSE}
library(rstan)
library(brms)


brmLogReg <- brm(formula = seedlingSurv ~ light,
               data = seedlingSurvData, family = bernoulli())

summary(brmLogReg)
plot(brmLogReg)


outStanLogReg <- brm(formula = seedlingSurv ~ light,
                data = seedlingSurvData, family = bernoulli(),
                prior = c(set_prior("normal(0,100)", class = "b", coef = "light")),
                #b means only fixed effects, which is what we want
               # prior = c(set_prior("normal(0,100)", class = "b")),
                warmup = 1000, iter = 2000, chains = 4)

summary(outStanLogReg)
plot(outStanLogReg)
plot(marginal_effects(outStanLogReg), points = TRUE)

```

Then, I used the \texttt{stanarm} library. Again, results were similar to the previous approaches. 
```{r}
library(rstanarm)

outrstanarmLogReg<-stan_glm(seedlingSurv ~ light, data = seedlingSurvData,
                      family = binomial, iter=2000, warmup=1000, cores=4)

summary(outrstanarmLogReg)
plot(outrstanarmLogReg)
```

Next, I used actual Stan code within the \texttt{rstan} library.
```{r, echo=FALSE}
#model
modelString="data {
int<lower=0> N;
real<lower=0,upper=1> light[N];
int<lower=0> seedlingSurv[N];
}

parameters {
// Define parameters to estimate
real beta0;
real beta1;
}

transformed parameters {
 // Probability trasformation from linear predictor
real<lower=0> odds[N];
real<lower=0, upper=1> prob[N];

for(i in 1:N){
odds[i] = exp(beta0 + beta1*light[i]);
prob[i] = odds[i] / (odds[i] + 1);
}
}

model {
// Prior part of Bayesian inference (flat if unspecified)
 // Likelihood part of Bayesian inference
seedlingSurv ~ bernoulli(prob);
}"

library(rstan)
SeedlingList = list(seedlingSurv = seedlingSurv, light = light, N = length(light))

stanLogReg = stan(model_code = modelString, data = SeedlingList,
                chains = 3, iter = 3000, warmup = 500, thin = 10)

summary(stanLogReg,par=c('beta0','beta1'))


library(coda)
post_fitLogReg = As.mcmc.list(stanLogReg) 
#plot(post_fitLogReg)

```

Lastly, I used the \texttt{rjags} library to fit the logistic regression model. The code is similar to the \texttt{rstan} model, but it is a little simpler. Again, the posterior distributions for the slope and intercept parameters show a high density around the actual parameters, and the trace plot (obtained via the \texttt{coda} library) shows that mixing within the MCMC chain was also sufficient 
```{r, echo=FALSE}
modelstring2 = "
model {
#likelihood

for (i in 1:N) {

seedlingSurv[i]~ dbern(mu[i])  

logit(mu[i]) <- beta0 + beta1 * light[i]
}
#priors for beta0 and beta1
beta0 ~ dnorm(0, 0.0001)
beta1 ~ dnorm(0, 0.0001)
}"
library(rjags)
writeLines(modelstring2, con='LRseedlings.jags.txt')

jagsLR <- jags.model('LRseedlings.jags.txt',
                   data = list('seedlingSurv' = seedlingSurv,
                               'light' = light,
                               'N' = length(light)),
                   inits<-list(
                     list('beta0'=1,'beta1'=2),
                     list('beta0'=1,'beta1'=.2),
                     list('beta0'=.1,'beta1'=2),
                     list('beta0'=2,'beta1'=-1)),
                   n.chains = 4,
                   n.adapt = 100)

update(jagsLR, 1000)

jags.samples(jagsLR,
             c('beta0', 'beta1'),
             1000)

library(coda)
codaSamplesLR=coda.samples(jagsLR, c('beta0','beta1'), 1000, 1)

plot(codaSamplesLR, trace = FALSE, density = TRUE)
summary(codaSamplesLR)
traceplot(codaSamplesLR)


```

